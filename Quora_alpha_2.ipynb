{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quora_alpha_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kutouxiyiji/other/blob/master/Quora_alpha_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRr0zGJSpkhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# continue with Quora_alpha_1, import libriary and mount google drive\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import pickle\n",
        "from xgboost import XGBClassifier as xgbc\n",
        "from sklearn.metrics import classification_report\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "set(stopwords.words('english'))\n",
        "\n",
        "nltk.download('punkt')\n",
        "from tqdm import tqdm_notebook\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from scipy.spatial.distance import *\n",
        "\n",
        "from scipy.stats import kurtosis\n",
        "from scipy.stats import skew"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PDPiV5DpxgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNuGB5P5p4pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv('/content/drive/My Drive/Python/dataSet/Quora/train_df.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCi59hBjqMho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_qs1 = pd.Series(df_train['question1'].astype('str').values)\n",
        "train_qs2 = pd.Series(df_train['question2'].astype('str').values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJCjUOzAzKNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_qs1[:3], train_qs2[:3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOVf9xbaqsfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cloud = WordCloud(width=1440, height=1080).generate(\" \".join(train_qs2.astype(str)))\n",
        "plt.figure(figsize=(20, 15))\n",
        "plt.imshow(cloud)\n",
        "plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cftDC6Ketd1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cloud = WordCloud(width=1440, height=1080).generate(\" \".join(train_qs1.astype(str)))\n",
        "plt.figure(figsize=(20, 15))\n",
        "plt.imshow(cloud)\n",
        "plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aia4_0lTq9Dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tvec = TfidfVectorizer(max_features = 1024,analyzer='word', stop_words='english')\n",
        "tvec.fit(train_qs1)\n",
        "X1 = tvec.transform(train_qs1)\n",
        "X2 = tvec.transform(train_qs2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DT4Jefqrpr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X1.shape, X2.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeRfh8HtrzdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = df_train['is_duplicate'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpG_h6HssHWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.mean(X1[0]),np.mean(X2[0]), np.mean(X1-X2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiaoBu8vz3FG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X1-X2).shape, y.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Qet7wr0RQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X1-X2, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njg75kGF0Xve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {}\n",
        "params['objective'] = 'binary:logistic'\n",
        "params['eval_metric'] = 'logloss'\n",
        "params['eta'] = 0.02\n",
        "params['max_depth'] = 4\n",
        "\n",
        "d_train = xgb.DMatrix(X_train, label=y_train)\n",
        "bst = xgb.train(params, d_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqZhfcAI2_nG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb_model = xgbc()\n",
        "xgb_model.fit(X= X_train,y = y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EDBV60t0pA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = xgb_model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEdyeiBc3mHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classification_report(y_test,y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOLIw1f04ofS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm=confusion_matrix(y_test,y_pred)\n",
        "df_cm = pd.DataFrame(cm, range(2),range(2))\n",
        "sns.set(font_scale=1.4)#for label size\n",
        "sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWgBJgulDdX9",
        "colab_type": "text"
      },
      "source": [
        "there is a serious problem. The recall for duplicated class is very low. We try undersample the non-duplicated class first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBiwz9mC8wsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_train = X_train[y_train == 1]\n",
        "neg_train = X_train[y_train == 0]\n",
        "pos_train.shape, neg_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6fqfODI80Pk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_pos_train = df_train[df_train['is_duplicate']==1]\n",
        "df_neg_train = df_train[df_train['is_duplicate']==0]\n",
        "len(df_pos_train),len(df_neg_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRIBqSUfB6jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_neg_train_under = df_neg_train.sample(len(df_pos_train))\n",
        "len(df_neg_train_under)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4yI_GARCMuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train_under = pd.concat([df_neg_train_under,df_pos_train], axis=0)\n",
        "len(df_train_under)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFi2MSfLCbdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_qs1_under = pd.Series(df_train_under['question1'].astype('str').values)\n",
        "train_qs2_under = pd.Series(df_train_under['question2'].astype('str').values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lov3IS26CjlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tvec = TfidfVectorizer(max_features = 1024,analyzer='word', stop_words='english')\n",
        "tvec.fit(train_qs1_under)\n",
        "X1 = tvec.transform(train_qs1_under)\n",
        "X2 = tvec.transform(train_qs2_under)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is-dDWIECtA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = df_train_under['is_duplicate'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e27zbzoBCvno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X1-X2).shape, y.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_1y4U_DDB4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X1-X2, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHxVw_tuDIei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb_model = xgbc()\n",
        "xgb_model.fit(X= X_train,y = y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb5tLX5LDMUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = xgb_model.predict(X_test)\n",
        "classification_report(y_test,y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p0C3QjxDZY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm=confusion_matrix(y_test,y_pred)\n",
        "df_cm = pd.DataFrame(cm, range(2),range(2))\n",
        "sns.set(font_scale=1.4)#for label size\n",
        "sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QYWl7ihhgrl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "try differnt vecrization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf8B5glghfsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tvec2 = TfidfVectorizer(max_features = 128,analyzer='word', stop_words='english',max_df = 0.95, min_df=0.005)\n",
        "tvec2.fit(train_qs1_under)\n",
        "X1_2 = tvec2.transform(train_qs1_under)\n",
        "X2_2 = tvec2.transform(train_qs2_under)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X1_2-X2_2, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shAwLbVyh82M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb_model = xgbc()\n",
        "xgb_model.fit(X= X_train,y = y_train)\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "classification_report(y_test,y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXEcYglNiBfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame()\n",
        "df_X1 = pd.DataFrame(X1_2.toarray(), columns=tvec2.get_feature_names())\n",
        "df_X2 = pd.DataFrame(X2_2.toarray())\n",
        "df = pd.concat([df_X1, df_X2], axis=1)\n",
        "# need re-run this cell if you already run the xgb model\n",
        "df['question1'] = df_train_under['question1']\n",
        "df['question2'] = df_train_under['question2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVKeOfJg3ort",
        "colab_type": "text"
      },
      "source": [
        "we use the df as X"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DndM21wp7y7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#(df_X1-df_X2).head()\n",
        "#df2 = pd.DataFrame()\n",
        "#df2=df_X1-df_X2\n",
        "#df2.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC85iq2M15Vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "len(df_X1), len(df_X2), len(df), df_X1.head(), df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDTkP26r15bE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add features\n",
        "df['len_q1'] = df.question1.apply(lambda x: len(str(x)))\n",
        "df['len_q2'] = df.question2.apply(lambda x: len(str(x)))\n",
        "df['diff_len'] = df.len_q1 - df.len_q2\n",
        "df['len_char_q1'] = df.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
        "df['len_char_q2'] = df.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
        "df['len_word_q1'] = df.question1.apply(lambda x: len(str(x).split()))\n",
        "df['len_word_q2'] = df.question2.apply(lambda x: len(str(x).split()))\n",
        "df['common_words'] = df.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t85JyWCt5ewK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxs0-2EJ5e4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWY9FHax5e_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfharkOu5e7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNmMIuKVJHrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAvHKcEb5e2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOJLYa-75e0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(['question1', 'question2'], axis=1, inplace=True)\n",
        "X = df\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "xgb_model = xgbc()\n",
        "xgb_model.fit(X= X_train,y = y_train)\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "classification_report(y_test,y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19kKULlF66F7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JMGxVyllPiQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Try word2vec\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-mpidsEmcpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wmd(q1, q2):\n",
        "    q1 = str(q1).lower().split()\n",
        "    q2 = str(q2).lower().split()\n",
        "    stop_words = stopwords.words('english')\n",
        "    q1 = [w for w in q1 if w not in stop_words]\n",
        "    q2 = [w for w in q2 if w not in stop_words]\n",
        "    return model.wmdistance(q1, q2)\n",
        "    \n",
        "def norm_wmd(q1, q2):\n",
        "    q1 = str(q1).lower().split()\n",
        "    q2 = str(q2).lower().split()\n",
        "    stop_words = stopwords.words('english')\n",
        "    q1 = [w for w in q1 if w not in stop_words]\n",
        "    q2 = [w for w in q2 if w not in stop_words]\n",
        "    return norm_model.wmdistance(q1, q2)\n",
        "    \n",
        "def sent2vec(s):\n",
        "    words = str(s).lower()\n",
        "    words = word_tokenize(words)\n",
        "    stop_words = stopwords.words('english')\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    words = [w for w in words if w.isalpha()]\n",
        "    M = []\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(model[w])\n",
        "        except:\n",
        "            continue\n",
        "    M = np.array(M)\n",
        "    v = M.sum(axis=0)\n",
        "    return v / np.sqrt((v ** 2).sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isbgbjaR_hnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDEagV2eJ9MM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/My Drive/Python/dataSet/Quora/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "#df['wmd'] = df.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIJ-13TKMQjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.init_sims(replace=True)\n",
        "df['wmd'] = df.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij9xHsTCO9GN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "norm_model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/My Drive/Python/dataSet/Quora/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "norm_model.init_sims(replace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tML14uaEJ9P-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "question1_vectors = np.zeros((df.shape[0], 300))\n",
        "for i, q in enumerate(tqdm_notebook(df.question1.values)):\n",
        "    question1_vectors[i, :] = sent2vec(q)\n",
        "    \n",
        "question2_vectors  = np.zeros((df.shape[0], 300))\n",
        "for i, q in enumerate(tqdm_notebook(df.question2.values)):\n",
        "    question2_vectors[i, :] = sent2vec(q)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVUYLSxYXIAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n",
        "df['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n",
        "df['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n",
        "df['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n",
        "df['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n",
        "df['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n",
        "df['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n",
        "df['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
        "df['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
        "df['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
        "df['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLxccAIaS4MJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(['question1', 'question2'], axis=1, inplace=True)\n",
        "#df = df[pd.notnull(df['cosine_distance'])]\n",
        "#df = df[pd.notnull(df['jaccard_distance'])]\n",
        "X = df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfXXdPSsa5Z3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = df_train_under['is_duplicate'].values\n",
        "X.shape, y.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhfMLUj5bjp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QTxjv9aap9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "model_xgb2 = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W8HYjYHbIKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_xgb2.fit(X= X_train,y = y_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILLI9LK_nBnC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = '/content/drive/My Drive/Python/dataSet/Quora/word2vec_1.model'\n",
        "pickle.dump(model_xgb2, open(filename, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUw5IcfUa0w8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = model_xgb2.predict(X_test)\n",
        "cm = confusion_matrix(y_test, prediction)  \n",
        "print(cm)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyLyQjF5nxqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cm = pd.DataFrame(cm, range(2),range(2))\n",
        "sns.set(font_scale=1.4)#for label size\n",
        "sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov8Ex7ALn79g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classification_report(y_test,prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDcVT5Vynxuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Accuracy', accuracy_score(y_test, prediction))\n",
        "print(classification_report(y_test, prediction))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UztUSyHf_hhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''df.drop(['question1', 'question2'], axis=1, inplace=True)\n",
        "X = df\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "xgb_model = xgbc()\n",
        "xgb_model.fit(X= X_train,y = y_train)\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "classification_report(y_test,y_pred)'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}